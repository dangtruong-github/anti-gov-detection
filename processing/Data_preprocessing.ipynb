{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNVXIHODrL_L"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7dF3FzOartF",
        "outputId": "33886205-77d4-487e-c728-81e19ded0288"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QO_QMn8u0NU"
      },
      "outputs": [],
      "source": [
        "# zip_file_path = '/content/drive/MyDrive/dataset/word2vec_vi_words_300dims.zip'\n",
        "import zipfile\n",
        "import os\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# extraction_path = '/content/drive/My Drive/word2vec_VN'\n",
        "\n",
        "# if not os.path.exists(extraction_path):\n",
        "#     os.makedirs(extraction_path)Ư\n",
        "\n",
        "word2vec_glove_file = get_tmpfile(\"/content/drive/MyDrive/dataset/word2vec_vi_words_300dims.txt\")\n",
        "\n",
        "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MtjbF7utdKd"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/MyDrive/word2vec')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7KyCxlo_oc0",
        "outputId": "ba3de79d-18f7-4cea-8b5f-cdf531dac594"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Nhà_báo', 0.789657473564148), ('phóng_viên', 0.7135041952133179), ('Washington_Post_Jamal_Khashoggi', 0.6566924452781677), ('Kosuke_Tsuneoka', 0.6515445709228516), ('Saudi_Arabia_Khashoggi', 0.6459189057350159), ('Arab_Saudi_Khashoggi', 0.64432293176651), ('J._Khashoggi', 0.6441531777381897), ('Anastasia_Baburova', 0.6433132886886597), ('báo_chí', 0.6406913995742798), ('Andrew_Simmons', 0.6366214752197266)]\n",
            "[('chống_phá', 0.7964444160461426), ('thù_địch', 0.6942732334136963), ('xuyên_tạc', 0.6806550025939941), ('phản_cách_mạng', 0.6742035746574402), ('luận_điệu', 0.6596599221229553), ('kích_động', 0.6521998047828674), ('Việt_Tân', 0.6492446660995483), ('PĐLV', 0.6437748670578003), ('diễn_biến_hoà_bình', 0.6317404508590698), ('dụnginternet', 0.6302641034126282)]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import collections\n",
        "import random\n",
        "import re\n",
        "import torch\n",
        "print(model.most_similar('nhà_báo'))\n",
        "print(model.most_similar('phản_động'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLfltJHGDpGR"
      },
      "outputs": [],
      "source": [
        "class Vocab:\n",
        "    \"\"\"Vocabulary for text.\"\"\"\n",
        "    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n",
        "        # Flatten a 2D list if needed\n",
        "        if tokens and isinstance(tokens[0], list):\n",
        "            tokens = [token for line in tokens for token in line]\n",
        "        # Count token frequencies\n",
        "        counter = collections.Counter(tokens)\n",
        "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
        "                                  reverse=True)\n",
        "        # The list of unique tokens\n",
        "        self.idx_to_token = list(sorted(set(['<unk>'] + reserved_tokens + [\n",
        "            token for token, freq in self.token_freqs if freq >= min_freq])))\n",
        "        self.token_to_idx = {token: idx\n",
        "                             for idx, token in enumerate(self.idx_to_token)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx_to_token)\n",
        "\n",
        "    def __getitem__(self, tokens):\n",
        "        if not isinstance(tokens, (list, tuple)):\n",
        "            return self.token_to_idx.get(tokens, self.unk)\n",
        "        return [self.__getitem__(token) for token in tokens]\n",
        "\n",
        "    def to_tokens(self, indices):\n",
        "        if hasattr(indices, '__len__') and len(indices) > 1:\n",
        "            return [self.idx_to_token[int(index)] for index in indices]\n",
        "        return self.idx_to_token[indices]\n",
        "\n",
        "    @property\n",
        "    def unk(self):  # Index for the unknown token\n",
        "        return self.token_to_idx['<unk>']\n",
        "\n",
        "def text_analytic(df,show_most_common = False,show_least_common = False,is_plot = False,\n",
        "                  check_bigram = False,check_trigram = False,check_5gram = False):\n",
        "    all_comment = ' '.join(df['text'])\n",
        "    word = all_comment.split()\n",
        "    print(len(word))\n",
        "    vocab = Vocab(word)\n",
        "\n",
        "    print(f'Vocab length is : {len(vocab.token_freqs)}')\n",
        "    arr = []\n",
        "    if (show_most_common):\n",
        "        for i in range (0,min(301,len(vocab.token_freqs))):\n",
        "            print(f'{i + 1}th most common word is : {vocab.token_freqs[i]}')\n",
        "            arr.append(vocab.token_freqs[i])\n",
        "\n",
        "        print('-------------------------------------')\n",
        "\n",
        "    if (show_least_common):\n",
        "        for i in range (len(vocab.token_freqs) - 1,len(vocab.token_freqs) - min(101,len(vocab.token_freqs)),-1):\n",
        "            print(f'{len(vocab.token_freqs) - i}th least common word is : {vocab.token_freqs[i]}')\n",
        "\n",
        "    if (is_plot):\n",
        "        x = np.arange(0,1000,1)\n",
        "        y = np.array(vocab.token_freqs[:1000])\n",
        "        y = y[:,1].astype(int)\n",
        "        plt.title('Word distribution among comment')\n",
        "        plt.xlabel('Word')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.plot(x,y)\n",
        "\n",
        "    if (check_bigram):\n",
        "        bigram_tokens = ['--'.join(pair) for pair in zip(word[:-1], word[1:])]\n",
        "        bigram_vocab = Vocab(bigram_tokens)\n",
        "        for i in range (0,300):\n",
        "            print(f'{i + 1}th most common bigrams is : {bigram_vocab.token_freqs[i]}')\n",
        "\n",
        "        print('-------------------------------------')\n",
        "\n",
        "    if (check_trigram):\n",
        "        trigram_tokens = ['--'.join(triple) for triple in zip(\n",
        "        word[:-2], word[1:-1], word[2:])]\n",
        "        trigram_vocab = Vocab(trigram_tokens)\n",
        "        for i in range (0,300):\n",
        "            print(f'{i + 1}th most common trigrams is : {trigram_vocab.token_freqs[i]}')\n",
        "\n",
        "        print('-------------------------------------')\n",
        "\n",
        "    if (check_5gram):\n",
        "        trigram_tokens = ['--'.join(penta) for penta in zip(\n",
        "        word[:-4], word[1:-3], word[2:-2], word[3:-1], word[4:])]\n",
        "        trigram_vocab = Vocab(trigram_tokens)\n",
        "        for i in range (0,300):\n",
        "            print(f'{i + 1}th most common 5grams is : {trigram_vocab.token_freqs[i]}')\n",
        "\n",
        "        print('-------------------------------------')\n",
        "    return arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "zDrjq7SAEVc_",
        "outputId": "340004ee-8914-4db0-c731-a9be353fdb11"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5d571329f161>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/dataset/fixed_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "def concat(text):\n",
        "  return ' '.join(text)\n",
        "df = pd.read_csv('/content/drive/MyDrive/dataset/fixed_data.csv')\n",
        "print(df.columns)\n",
        "print(df.shape)\n",
        "print(df['text'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsJwuHPIg4i8",
        "outputId": "2143ed9c-9170-4b38-97e5-5e6f67f054ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 2, 3]\n"
          ]
        }
      ],
      "source": [
        "import ast\n",
        "\n",
        "string_list = \"[1, 2, 3]\"\n",
        "actual_list = ast.literal_eval(string_list)\n",
        "print(actual_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOuNAHQqCHA9"
      },
      "outputs": [],
      "source": [
        "def embedding_300(text):\n",
        "  text = ast.literal_eval(text)\n",
        "  n_size = len(text)\n",
        "  n_feature = 300\n",
        "  embed = []\n",
        "  if n_size <= n_feature:\n",
        "    for x in text:\n",
        "      cur = np.zeros(n_feature)\n",
        "      if model.has_index_for(x):\n",
        "        cur = model.get_vector(x)\n",
        "      embed.append(cur)\n",
        "    for _ in range(n_feature-n_size):\n",
        "      embed.append(model.get_vector('#'))\n",
        "  else:\n",
        "    for x in text[:(n_feature-100)//2]:\n",
        "      cur = np.zeros(n_feature)\n",
        "      if model.has_index_for(x):\n",
        "        cur = model.get_vector(x)\n",
        "      embed.append(cur)\n",
        "    s = (n_feature-100)//2 + (n_size-n_feature)//2\n",
        "    for x in text[s:s+100]:\n",
        "      cur = np.zeros(n_feature)\n",
        "      if model.has_index_for(x):\n",
        "        cur = model.get_vector(x)\n",
        "      embed.append(cur)\n",
        "    for x in text[-(n_feature-100)//2:]:\n",
        "      cur = np.zeros(n_feature)\n",
        "      if model.has_index_for(x):\n",
        "        cur = model.get_vector(x)\n",
        "      embed.append(cur)\n",
        "  arr = np.concatenate(embed)\n",
        "  # print(len(embed), arr.shape)\n",
        "  return arr\n",
        "df['embedding'] = df['text_split'].apply(embedding_300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULPyphgvIVOf"
      },
      "outputs": [],
      "source": [
        "list_of_lists = df['embedding'].tolist()\n",
        "transposed_list = list(map(list, zip(*list_of_lists)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lop8XWbUhjue"
      },
      "outputs": [],
      "source": [
        "# df['embedding'] = df['text_split'].apply(embedding_300)\n",
        "save_df = pd.DataFrame(transposed_list)\n",
        "\n",
        "save_df['label'] = df['label']\n",
        "print(save_df.shape)\n",
        "print(save_df.info())\n",
        "print(save_df.describe)\n",
        "save_df.to_csv('/content/drive/MyDrive/dataset/embedding_300.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWAhGb5U_Uu6"
      },
      "outputs": [],
      "source": [
        "def embedding_500(text):\n",
        "  text = ast.literal_eval(text)\n",
        "  # print(type(text))\n",
        "  n_size = len(text)\n",
        "  n_feature = 500\n",
        "  embed = []\n",
        "  if n_size <= n_feature:\n",
        "    for x in text:\n",
        "      cur = np.zeros(300)\n",
        "      if model.has_index_for(x):\n",
        "        cur = model.get_vector(x)\n",
        "        if len(cur) != 300:\n",
        "          print(x)\n",
        "          return False\n",
        "      embed.append(cur)\n",
        "    for _ in range(n_feature-n_size):\n",
        "      embed.append(model.get_vector('#'))\n",
        "  else:\n",
        "    for x in text[:(n_feature-100)//2]:\n",
        "      cur = np.zeros(300)\n",
        "      if model.has_index_for(x):\n",
        "        cur = model.get_vector(x)\n",
        "        if len(cur) != 300:\n",
        "          print(x)\n",
        "          return False\n",
        "      embed.append(cur)\n",
        "    s = (n_feature-100)//2 + (n_size-n_feature)//2\n",
        "    for x in text[s:s+100]:\n",
        "      cur = np.zeros(300)\n",
        "      if model.has_index_for(x):\n",
        "        cur = model.get_vector(x)\n",
        "        if len(cur) != 300:\n",
        "          print(x)\n",
        "          return False\n",
        "      embed.append(cur)\n",
        "    for x in text[-(n_feature-100)//2:]:\n",
        "      cur = np.zeros(300)\n",
        "      if model.has_index_for(x):\n",
        "        cur = model.get_vector(x)\n",
        "        if len(cur) != 300:\n",
        "          print(x)\n",
        "          return False\n",
        "      embed.append(cur)\n",
        "  # print(n_size, len(embed))\n",
        "  arr = np.concatenate(embed)\n",
        "  if len(arr) != 150000:\n",
        "    print(i)\n",
        "  return arr\n",
        "df['embedding'] = df['text_split'].apply(embedding_500)\n",
        "save_df = pd.DataFrame(df['embedding'].tolist())\n",
        "save_df['label'] = df['label']\n",
        "save_df.to_csv('/content/drive/MyDrive/dataset/embedding_500.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sh4bzlJ0IZZd"
      },
      "outputs": [],
      "source": [
        "def convert_dataloader(df, name, embedding_fn):\n",
        "  df['embedding'] = df['text_split'].apply(embedding_fn)\n",
        "  data_train, data_test, label_train, label_test = train_test_split(df, df['label'], test_size=0.2)\n",
        "  tensor_x = torch.Tensor(data_train['embedding'].tolist())\n",
        "  tensor_y = torch.Tensor(data_train['label'].tolist())\n",
        "  my_dataset = TensorDataset(tensor_x,tensor_y)\n",
        "  my_dataloader = DataLoader(my_dataset, batch_size=8)\n",
        "  # print(my_dataloader.shape)\n",
        "  joblib.dump(my_dataloader, f'/content/drive/MyDrive/trainloader_{name}')\n",
        "  tensor_x = torch.Tensor(data_test['embedding'].tolist())\n",
        "  tensor_y = torch.Tensor(data_test['label'].tolist())\n",
        "  my_dataset = TensorDataset(tensor_x,tensor_y)\n",
        "  my_dataloader = DataLoader(my_dataset, batch_size=8)\n",
        "  # print(my_dataloader.shape)\n",
        "  print(label_test.sum()/len(label_test))\n",
        "  print(label_train.sum()/len(label_train))\n",
        "  joblib.dump(my_dataloader, f'/content/drive/MyDrive/testloader_{name}')\n",
        "# convert_dataloader(df, '300', embedding_300)\n",
        "dropped_words = most_common_words[:50]\n",
        "for x in most_common_words[50:]:\n",
        "  if len(x) <= 1:\n",
        "    dropped_words.append(x)\n",
        "def drop_embedding_300(text):\n",
        "  text = ast.literal_eval(text)\n",
        "  text = [x for x in text if x not in dropped_words]\n",
        "  n_size = len(text)\n",
        "  n_feature = 300\n",
        "  embed = []\n",
        "  if n_size <= n_feature:\n",
        "    for x in text:\n",
        "      cur = np.zeros(n_feature)\n",
        "      if model.has_index_for(x):\n",
        "        cur = model.get_vector(x)\n",
        "      embed.append(cur)\n",
        "    for _ in range(n_feature-n_size):\n",
        "      embed.append(model.get_vector('#'))\n",
        "  else:\n",
        "    for x in text[:(n_feature-100)//2]:\n",
        "      cur = np.zeros(n_feature)\n",
        "      if model.has_index_for(x):\n",
        "        cur = model.get_vector(x)\n",
        "      embed.append(cur)\n",
        "    s = (n_feature-100)//2 + (n_size-n_feature)//2\n",
        "    for x in text[s:s+100]:\n",
        "      cur = np.zeros(n_feature)\n",
        "      if model.has_index_for(x):\n",
        "        cur = model.get_vector(x)\n",
        "      embed.append(cur)\n",
        "    for x in text[-(n_feature-100)//2:]:\n",
        "      cur = np.zeros(n_feature)\n",
        "      if model.has_index_for(x):\n",
        "        cur = model.get_vector(x)\n",
        "      embed.append(cur)\n",
        "  return np.array(embed)\n",
        "# convert_dataloader(df, '300_drop', drop_embedding_300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgqFVVJVLCbX",
        "outputId": "85e752f4-c9ab-42cf-a905-7e866185f1d8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-0a74bf744e9a>:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
            "  tensor_x = torch.Tensor(data_train['embedding'].tolist())\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.1419098143236074\n",
            "0.16312997347480107\n"
          ]
        }
      ],
      "source": [
        "# convert_dataloader(df, '300_drop', drop_embedding_300)\n",
        "def drop_embedding_500(text):\n",
        "  text = ast.literal_eval(text)\n",
        "  text = [x for x in text if x not in dropped_words]\n",
        "  # print(type(text))\n",
        "  n_size = len(text)\n",
        "  n_feature = 500\n",
        "  embed = []\n",
        "  if n_size <= n_feature:\n",
        "    for x in text:\n",
        "      cur = np.zeros(300)\n",
        "      if model.has_index_for(x):\n",
        "        cur = model.get_vector(x)\n",
        "        if len(cur) != 300:\n",
        "          print(x)\n",
        "          return False\n",
        "      embed.append(cur)\n",
        "    for _ in range(n_feature-n_size):\n",
        "      embed.append(model.get_vector('#'))\n",
        "  else:\n",
        "    for x in text[:(n_feature-100)//2]:\n",
        "      cur = np.zeros(300)\n",
        "      if model.has_index_for(x):\n",
        "        cur = model.get_vector(x)\n",
        "        if len(cur) != 300:\n",
        "          print(x)\n",
        "          return False\n",
        "      embed.append(cur)\n",
        "    s = (n_feature-100)//2 + (n_size-n_feature)//2\n",
        "    for x in text[s:s+100]:\n",
        "      cur = np.zeros(300)\n",
        "      if model.has_index_for(x):\n",
        "        cur = model.get_vector(x)\n",
        "        if len(cur) != 300:\n",
        "          print(x)\n",
        "          return False\n",
        "      embed.append(cur)\n",
        "    for x in text[-(n_feature-100)//2:]:\n",
        "      cur = np.zeros(300)\n",
        "      if model.has_index_for(x):\n",
        "        cur = model.get_vector(x)\n",
        "        if len(cur) != 300:\n",
        "          print(x)\n",
        "          return False\n",
        "      embed.append(cur)\n",
        "\n",
        "  return np.array(embed)\n",
        "convert_dataloader(df, '500_drop', drop_embedding_500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkfUahubg901"
      },
      "outputs": [],
      "source": [
        "for i in range(len(save_df)):\n",
        "  if len(save_df['embedding'][i]) != 300*500:\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjxGvtPIE5--"
      },
      "outputs": [],
      "source": [
        "def drop_embedding_500(text):\n",
        "  text = ast.literal_eval(text)\n",
        "  text = [x for x in text if x not in dropped_words]\n",
        "  # print(type(text))\n",
        "  n_size = len(text)\n",
        "  n_feature = 500\n",
        "  embed = []\n",
        "  if n_size <= n_feature:\n",
        "    for x in text:\n",
        "      cur = np.zeros(300)\n",
        "      if model.has_index_for(x):\n",
        "        cur = model.get_vector(x)\n",
        "        if len(cur) != 300:\n",
        "          print(x)\n",
        "          return False\n",
        "      embed.append(cur)\n",
        "    for _ in range(n_feature-n_size):\n",
        "      embed.append(model.get_vector('#'))\n",
        "  else:\n",
        "    for x in text[:(n_feature-100)//2]:\n",
        "      cur = np.zeros(300)\n",
        "      if model.has_index_for(x):\n",
        "        cur = model.get_vector(x)\n",
        "        if len(cur) != 300:\n",
        "          print(x)\n",
        "          return False\n",
        "      embed.append(cur)\n",
        "    s = (n_feature-100)//2 + (n_size-n_feature)//2\n",
        "    for x in text[s:s+100]:\n",
        "      cur = np.zeros(300)\n",
        "      if model.has_index_for(x):\n",
        "        cur = model.get_vector(x)\n",
        "        if len(cur) != 300:\n",
        "          print(x)\n",
        "          return False\n",
        "      embed.append(cur)\n",
        "    for x in text[-(n_feature-100)//2:]:\n",
        "      cur = np.zeros(300)\n",
        "      if model.has_index_for(x):\n",
        "        cur = model.get_vector(x)\n",
        "        if len(cur) != 300:\n",
        "          print(x)\n",
        "          return False\n",
        "      embed.append(cur)\n",
        "\n",
        "  arr = np.concatenate(embed)\n",
        "  if len(arr) != 300*500:\n",
        "    print(n_size, len(arr))\n",
        "  return arr\n",
        "convert_dataloader(df, '500_drop', drop_embedding_500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fs56OZkDKmPm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
